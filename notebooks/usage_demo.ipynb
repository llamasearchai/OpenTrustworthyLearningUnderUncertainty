{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenTLU Usage Demo\n",
    "\n",
    "This notebook demonstrates the core capabilities of the OpenTLU (Trustworthy Learning in Uncertainty) framework.\n",
    "\n",
    "We will cover:\n",
    "1.  **Safety Filtering**: Ensuring actions stay within safe constraints.\n",
    "2.  **Uncertainty Estimation**: Generating uncertainty metrics for predictions.\n",
    "3.  **Active Learning**: Selecting informative samples based on uncertainty and risk.\n",
    "4.  **Runtime Control**: Monitoring system health and handling Out-of-Distribution (OOD) events.\n",
    "5.  **Evaluation**: Rigorous statistical evaluation of system performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Safety Filtering\n",
    "\n",
    "We define a safety envelope using box constraints and linear constraints, then use the `SafetyFilter` to correct unsafe actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from opentlu.safety.filter import SafetyFilter, BoxConstraint, LinearConstraint\n",
    "from opentlu.foundations.contracts import SafetyEnvelope\n",
    "\n",
    "# Define constraints\n",
    "# Action must be between [-1, 1]\n",
    "box = BoxConstraint(lower=np.array([-1.0]), upper=np.array([1.0]))\n",
    "\n",
    "# Initialize Safety Filter\n",
    "envelope = SafetyEnvelope(source=\"demo_envelope\", constraints=[\"box_limit\"])\n",
    "safety_filter = SafetyFilter(envelope=envelope, box_constraints=[box])\n",
    "\n",
    "# Test Safe Action\n",
    "safe_action = np.array([0.5])\n",
    "res_safe = safety_filter.filter(safe_action)\n",
    "print(f\"Original: {safe_action}, Filtered: {res_safe.action}, Modified: {res_safe.was_modified}\")\n",
    "\n",
    "# Test Unsafe Action (Outside bounds)\n",
    "unsafe_action = np.array([2.5])\n",
    "res_unsafe = safety_filter.filter(unsafe_action)\n",
    "print(f\"Original: {unsafe_action}, Filtered: {res_unsafe.action}, Modified: {res_unsafe.was_modified}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Uncertainty & Risk Estimation\n",
    "\n",
    "We simulate uncertainty estimates (aleatoric vs epistemic) and assess risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentlu.foundations.contracts import UncertaintyEstimate, RiskAssessment\n",
    "\n",
    "# Simulate high epistemic uncertainty (model doesn't know)\n",
    "unc_est = UncertaintyEstimate(\n",
    "    confidence=0.4,\n",
    "    aleatoric_score=0.2,\n",
    "    epistemic_score=0.8,\n",
    "    source=\"ensemble_model\"\n",
    ")\n",
    "\n",
    "# Define risk assessment\n",
    "risk = RiskAssessment(\n",
    "    expected_risk=0.5,\n",
    "    tail_risk_cvar=0.8,\n",
    "    violation_probability=0.1,\n",
    "    is_acceptable=False\n",
    ")\n",
    "\n",
    "print(f\"Uncertainty: {unc_est}\")\n",
    "print(f\"Risk Assessment: {risk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Active Learning\n",
    "\n",
    "Use the `DataAcquisitionPolicy` to select the most important samples for labeling based on uncertainty, risk, and novelty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentlu.active_learning.acquisition import DataAcquisitionPolicy, AcquisitionConfig, SampleMetadata\n",
    "\n",
    "# Configure policy to prioritize high uncertainty and high risk\n",
    "config = AcquisitionConfig(weight_uncertainty=1.0, weight_risk=1.0, weight_novelty=0.5)\n",
    "policy = DataAcquisitionPolicy(config)\n",
    "\n",
    "# Create dummy samples\n",
    "def make_sample(id, unc_score, risk_score):\n",
    "    u = UncertaintyEstimate(\n",
    "        confidence=0.5,\n",
    "        aleatoric_score=0.1,\n",
    "        epistemic_score=unc_score,\n",
    "        source=\"test\"\n",
    "    )\n",
    "    r = RiskAssessment(\n",
    "        expected_risk=risk_score,\n",
    "        tail_risk_cvar=0.0,\n",
    "        violation_probability=0.0,\n",
    "        is_acceptable=True\n",
    "    )\n",
    "    return SampleMetadata(id, u, r, novelty_score=0.0)\n",
    "\n",
    "samples = [\n",
    "    make_sample(\"s1_low\", 0.1, 0.1),\n",
    "    make_sample(\"s2_med\", 0.5, 0.5),\n",
    "    make_sample(\"s3_high\", 0.9, 0.9)\n",
    "]\n",
    "\n",
    "scores = policy.compute_scores(samples)\n",
    "selected = policy.select_batch(samples, batch_size=1)\n",
    "\n",
    "print(\"Scores:\", scores)\n",
    "print(\"Selected Sample:\", selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Runtime Control & OOD Detection\n",
    "\n",
    "Monitor the system for health issues or Out-of-Distribution events and trigger mitigations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentlu.runtime.controller import MitigationController\n",
    "from opentlu.runtime.ood import MahalanobisDetector\n",
    "from opentlu.foundations.contracts import MitigationState\n",
    "\n",
    "# Initialize Controller\n",
    "controller = MitigationController(monitors=[])\n",
    "\n",
    "# Simulate Normal Operation\n",
    "state_nominal = controller.step({}, unc_est, ood_score=0.2)\n",
    "print(f\"State (Normal OOD): {state_nominal}\")\n",
    "\n",
    "# Simulate High OOD\n",
    "state_fallback = controller.step({}, unc_est, ood_score=0.95)\n",
    "print(f\"State (High OOD): {state_fallback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Evaluation\n",
    "\n",
    "Evaluate model performance with rigorous confidence intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opentlu.evaluation.statistics import StatisticalEvaluator\n",
    "\n",
    "# Evaluator with threshold for acceptance\n",
    "evaluator = StatisticalEvaluator(acceptance_thresholds={\"accuracy\": 0.9})\n",
    "\n",
    "# Simulate results\n",
    "results = [\n",
    "    {\"metrics\": {\"accuracy\": 0.95}, \"passed\": True, \"tags\": {\"weather\": \"clear\"}},\n",
    "    {\"metrics\": {\"accuracy\": 0.92}, \"passed\": True, \"tags\": {\"weather\": \"clear\"}},\n",
    "    {\"metrics\": {\"accuracy\": 0.60}, \"passed\": False, \"tags\": {\"weather\": \"rain\"}}\n",
    "]\n",
    "\n",
    "agg = evaluator.aggregate_results(results, stratify_by=[\"weather\"])\n",
    "\n",
    "print(f\"Overall Pass Rate: {agg.pass_rate.value:.2f} [{agg.pass_rate.ci_lower:.2f}, {agg.pass_rate.ci_upper:.2f}]\")\n",
    "print(\"Stratified Accuracy (Rain):\", agg.stratified_metrics[\"weather\"].strata[\"rain\"][\"accuracy\"].value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
